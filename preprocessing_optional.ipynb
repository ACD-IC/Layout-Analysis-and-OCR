{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0be2caa2-9e49-4f77-9abd-6fe31e126bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242038e-4452-479c-b660-1a9f43a60825",
   "metadata": {},
   "source": [
    "Image preprocessing for extracted PNG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1cf2a2e-f9c6-488e-96b7-c14fce8aea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# CORE PREPROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def detect_quality_and_skip(image_path: Path) -> bool:\n",
    "    \"\"\"Quick check if image needs preprocessing - early exit for clean scans\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return False\n",
    "            \n",
    "        # Check contrast (low contrast = needs enhancement)\n",
    "        contrast = img.std()\n",
    "        if contrast > 60:  # High contrast = likely clean scan\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def enhance_contrast_clahe(img: np.ndarray, clip_limit: float = 2.0) -> np.ndarray:\n",
    "    \"\"\"Apply CLAHE for local contrast enhancement\"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "    return clahe.apply(img)\n",
    "\n",
    "def denoise_image(img: np.ndarray, strength: int = 3) -> np.ndarray:\n",
    "    \"\"\"Remove noise from historical scans\"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        return cv2.fastNlMeansDenoisingColored(img, None, strength, strength, 7, 21)\n",
    "    else:\n",
    "        return cv2.fastNlMeansDenoising(img, None, strength, 7, 21)\n",
    "\n",
    "def deskew_image(img: np.ndarray, max_angle: float = 15.0) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Deskew historical documents\"\"\"\n",
    "    gray = img if len(img.shape) == 2 else cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Canny + Hough for robust line detection\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=100, maxLineGap=10)\n",
    "    \n",
    "    angles = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            if x2 - x1 != 0:\n",
    "                angle = np.arctan2(y2 - y1, x2 - x1) * 180.0 / np.pi\n",
    "                # Normalize to -45 to 45\n",
    "                if angle > 45: angle -= 90\n",
    "                if angle < -45: angle += 90\n",
    "                if abs(angle) <= max_angle:\n",
    "                    angles.append(angle)\n",
    "    \n",
    "    if not angles:\n",
    "        return img, 0.0\n",
    "    \n",
    "    # Use median for robustness\n",
    "    final_angle = np.median(angles)\n",
    "    \n",
    "    if abs(final_angle) < 0.3:  # Skip tiny rotations\n",
    "        return img, final_angle\n",
    "    \n",
    "    # Rotate image\n",
    "    h, w = img.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, final_angle, 1.0)\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    cos = abs(M[0, 0])\n",
    "    sin = abs(M[0, 1])\n",
    "    new_w = int(h * sin + w * cos)\n",
    "    new_h = int(h * cos + w * sin)\n",
    "    \n",
    "    M[0, 2] += (new_w / 2) - center[0]\n",
    "    M[1, 2] += (new_h / 2) - center[1]\n",
    "    \n",
    "    rotated = cv2.warpAffine(img, M, (new_w, new_h), \n",
    "                            borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "    \n",
    "    return rotated, final_angle\n",
    "\n",
    "def adaptive_threshold_historical(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Adaptive thresholding optimized for historical documents\"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img.copy()\n",
    "    \n",
    "    # Bilateral filter for edge-preserving smoothing\n",
    "    filtered = cv2.bilateralFilter(gray, 9, 80, 80)\n",
    "    \n",
    "    # Adaptive threshold with larger block size for historical docs\n",
    "    binary = cv2.adaptiveThreshold(filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                  cv2.THRESH_BINARY, 15, 8)\n",
    "    \n",
    "    # Light morphological cleaning\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def remove_shadows(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Remove shadows from book spine/binding\"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img.copy()\n",
    "    \n",
    "    # Create background model using dilation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
    "    background = cv2.morphologyEx(gray, cv2.MORPH_DILATE, kernel)\n",
    "    \n",
    "    # Subtract background\n",
    "    result = cv2.divide(gray, background, scale=255)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def process_historical_image(input_path: Path, output_path: Path, options: dict) -> dict:\n",
    "    \"\"\"Process a single historical document image\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        img = cv2.imread(str(input_path))\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image: {input_path}\")\n",
    "        \n",
    "        steps = []\n",
    "        \n",
    "        # Early exit for clean images (optional)\n",
    "        if options.get('early_exit', True):\n",
    "            if detect_quality_and_skip(input_path):\n",
    "                # Just copy the file\n",
    "                Image.open(input_path).save(output_path, format='PNG', optimize=True)\n",
    "                return {'success': True, 'steps': ['copied_clean'], 'time_ms': 0}\n",
    "        \n",
    "        # Apply preprocessing steps\n",
    "        if options.get('denoise', True):\n",
    "            img = denoise_image(img, strength=options.get('denoise_strength', 3))\n",
    "            steps.append('denoise')\n",
    "        \n",
    "        if options.get('remove_shadows', True):\n",
    "            img = remove_shadows(img)\n",
    "            steps.append('shadow_removal')\n",
    "        \n",
    "        if options.get('clahe', True):\n",
    "            img = enhance_contrast_clahe(img, clip_limit=options.get('clahe_limit', 2.0))\n",
    "            steps.append('clahe')\n",
    "        \n",
    "        if options.get('deskew', True):\n",
    "            img, angle = deskew_image(img, max_angle=options.get('max_angle', 15.0))\n",
    "            if abs(angle) > 0.3:\n",
    "                steps.append(f'deskew_{angle:.1f}deg')\n",
    "        \n",
    "        if options.get('threshold', True):\n",
    "            img = adaptive_threshold_historical(img)\n",
    "            steps.append('threshold')\n",
    "        \n",
    "        # Save result\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        cv2.imwrite(str(output_path), img)\n",
    "        \n",
    "        processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'steps': steps,\n",
    "            'time_ms': processing_time,\n",
    "            'input_file': str(input_path),\n",
    "            'output_file': str(output_path)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {input_path}: {e}\")\n",
    "        return {'success': False, 'error': str(e), 'input_file': str(input_path)}\n",
    "\n",
    "# =============================================================================\n",
    "# EDITION PROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def find_edition_folders(editions_dir: Path) -> List[str]:\n",
    "    \"\"\"Find all edition folders in Editions directory\"\"\"\n",
    "    if not editions_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    edition_folders = []\n",
    "    for item in editions_dir.iterdir():\n",
    "        if item.is_dir() and not item.name.endswith('_processed'):\n",
    "            edition_folders.append(item.name)\n",
    "    \n",
    "    return sorted(edition_folders)\n",
    "\n",
    "def get_image_files(edition_path: Path) -> List[Path]:\n",
    "    \"\"\"Get all image files from an edition folder\"\"\"\n",
    "    extensions = {'.png', '.jpg', '.jpeg', '.tif', '.tiff'}\n",
    "    \n",
    "    image_files = []\n",
    "    for file_path in edition_path.iterdir():\n",
    "        if file_path.is_file() and file_path.suffix.lower() in extensions:\n",
    "            image_files.append(file_path)\n",
    "    \n",
    "    return sorted(image_files)\n",
    "\n",
    "def process_edition(editions_dir: Path, edition_name: str, options: dict = None, max_workers: int = 4) -> dict:\n",
    "    \"\"\"Process a single edition folder\"\"\"\n",
    "    \n",
    "    if options is None:\n",
    "        options = {\n",
    "            'early_exit': True,\n",
    "            'denoise': True,\n",
    "            'denoise_strength': 3,\n",
    "            'remove_shadows': True,\n",
    "            'clahe': True,\n",
    "            'clahe_limit': 2.0,\n",
    "            'deskew': True,\n",
    "            'max_angle': 15.0,\n",
    "            'threshold': True\n",
    "        }\n",
    "    \n",
    "    # Set up paths\n",
    "    input_dir = editions_dir / edition_name\n",
    "    output_dir = editions_dir / f\"{edition_name}_processed\"\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        logger.error(f\"Edition folder does not exist: {input_dir}\")\n",
    "        return {'success': False, 'error': 'Edition folder not found'}\n",
    "    \n",
    "    # Find all images\n",
    "    image_files = get_image_files(input_dir)\n",
    "    \n",
    "    if not image_files:\n",
    "        logger.warning(f\"No image files found in {input_dir}\")\n",
    "        return {'success': True, 'total': 0, 'processed': 0, 'skipped': 0, 'failed': 0}\n",
    "    \n",
    "    logger.info(f\"Processing edition '{edition_name}' with {len(image_files)} images\")\n",
    "    logger.info(f\"Input: {input_dir}\")\n",
    "    logger.info(f\"Output: {output_dir}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Process images in parallel\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            output_path = output_dir / img_path.name\n",
    "            future = executor.submit(process_historical_image, img_path, output_path, options)\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Collect results\n",
    "        for i, future in enumerate(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                if i % 10 == 0:  # Progress update every 10 files\n",
    "                    logger.info(f\"Progress: {i+1}/{len(image_files)} files processed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process image: {e}\")\n",
    "                results.append({'success': False, 'error': str(e)})\n",
    "    \n",
    "    # Calculate summary\n",
    "    total = len(results)\n",
    "    successful = sum(1 for r in results if r.get('success', False))\n",
    "    skipped = sum(1 for r in results if r.get('success', False) and 'copied_clean' in r.get('steps', []))\n",
    "    processed = successful - skipped\n",
    "    failed = total - successful\n",
    "    \n",
    "    summary = {\n",
    "        'success': True,\n",
    "        'edition': edition_name,\n",
    "        'total': total,\n",
    "        'processed': processed,\n",
    "        'skipped': skipped,\n",
    "        'failed': failed,\n",
    "        'output_dir': str(output_dir),\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Edition '{edition_name}' complete:\")\n",
    "    logger.info(f\"  Total files: {total}\")\n",
    "    logger.info(f\"  Processed: {processed}\")\n",
    "    logger.info(f\"  Skipped (clean): {skipped}\")\n",
    "    logger.info(f\"  Failed: {failed}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def process_all_editions(editions_dir: Path, options: dict = None, max_workers: int = 4) -> dict:\n",
    "    \"\"\"Process all edition folders in the Editions directory\"\"\"\n",
    "    \n",
    "    edition_folders = find_edition_folders(editions_dir)\n",
    "    \n",
    "    if not edition_folders:\n",
    "        logger.warning(f\"No edition folders found in {editions_dir}\")\n",
    "        return {'success': False, 'error': 'No edition folders found'}\n",
    "    \n",
    "    logger.info(f\"Found {len(edition_folders)} editions to process:\")\n",
    "    for edition in edition_folders:\n",
    "        logger.info(f\"  - {edition}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    total_processed = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    for edition_name in edition_folders:\n",
    "        logger.info(f\"\\nStarting edition: {edition_name}\")\n",
    "        result = process_edition(editions_dir, edition_name, options, max_workers)\n",
    "        all_results[edition_name] = result\n",
    "        \n",
    "        if result.get('success', False):\n",
    "            total_processed += result.get('processed', 0)\n",
    "            total_files += result.get('total', 0)\n",
    "    \n",
    "    logger.info(f\"\\nAll editions complete:\")\n",
    "    logger.info(f\"  Total editions: {len(edition_folders)}\")\n",
    "    logger.info(f\"  Total files processed: {total_processed}/{total_files}\")\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'editions_processed': len(edition_folders),\n",
    "        'total_files': total_files,\n",
    "        'total_processed': total_processed,\n",
    "        'results': all_results\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# CONVENIENCE FUNCTIONS FOR NOTEBOOK USE\n",
    "# =============================================================================\n",
    "\n",
    "def process_single_edition(edition_name: str, editions_dir: str = \"Editions\"):\n",
    "    \"\"\"Process a single edition with default historical document settings\"\"\"\n",
    "    editions_path = Path(editions_dir)\n",
    "    \n",
    "    if not editions_path.exists():\n",
    "        print(f\"Error: {editions_dir} directory does not exist\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing edition: {edition_name}\")\n",
    "    print(f\"Location: {editions_path / edition_name}\")\n",
    "    \n",
    "    result = process_edition(editions_path, edition_name)\n",
    "    \n",
    "    if result.get('success', False):\n",
    "        print(f\"Complete - Processed: {result['processed']}, Skipped: {result['skipped']}, Failed: {result['failed']}\")\n",
    "        print(f\"Output saved to: {result['output_dir']}\")\n",
    "    else:\n",
    "        print(f\"Failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def process_single_edition_custom(edition_name: str, options: dict, editions_dir: str = \"Editions\"):\n",
    "    \"\"\"Process a single edition with custom options\"\"\"\n",
    "    editions_path = Path(editions_dir)\n",
    "    \n",
    "    if not editions_path.exists():\n",
    "        print(f\"Error: {editions_dir} directory does not exist\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing edition: {edition_name}\")\n",
    "    print(f\"Custom settings: {options}\")\n",
    "    \n",
    "    result = process_edition(editions_path, edition_name, options)\n",
    "    \n",
    "    if result.get('success', False):\n",
    "        print(f\"Complete - Processed: {result['processed']}, Skipped: {result['skipped']}, Failed: {result['failed']}\")\n",
    "        print(f\"Output saved to: {result['output_dir']}\")\n",
    "    else:\n",
    "        print(f\"Failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def process_all(editions_dir: str = \"Editions\"):\n",
    "    \"\"\"Process all editions with default historical document settings\"\"\"\n",
    "    editions_path = Path(editions_dir)\n",
    "    \n",
    "    if not editions_path.exists():\n",
    "        print(f\"Error: {editions_dir} directory does not exist\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing all editions in: {editions_path}\")\n",
    "    \n",
    "    result = process_all_editions(editions_path)\n",
    "    \n",
    "    if result.get('success', False):\n",
    "        print(f\"All editions complete:\")\n",
    "        print(f\"  Editions processed: {result['editions_processed']}\")\n",
    "        print(f\"  Total files processed: {result['total_processed']}/{result['total_files']}\")\n",
    "    else:\n",
    "        print(f\"Failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f93d3d6-23f0-421b-aa6c-f69e3c5a7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 19:53:26,058 - INFO - Processing edition 'Padova_1618_Cesare_Ripa' with 704 images\n",
      "2025-09-08 19:53:26,059 - INFO - Input: Editions/Padova_1618_Cesare_Ripa\n",
      "2025-09-08 19:53:26,059 - INFO - Output: Editions/Padova_1618_Cesare_Ripa_processed\n",
      "Exception ignored in: <function tqdm.__del__ at 0x11b44fc70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/icon/lib/python3.10/site-packages/tqdm/std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/icon/lib/python3.10/site-packages/tqdm/notebook.py\", line 288, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "2025-09-08 19:53:26,280 - INFO - Progress: 1/704 files processed\n",
      "2025-09-08 19:53:26,401 - INFO - Progress: 11/704 files processed\n",
      "2025-09-08 19:53:27,669 - INFO - Progress: 21/704 files processed\n",
      "2025-09-08 19:53:27,670 - INFO - Progress: 31/704 files processed\n",
      "2025-09-08 19:53:27,670 - INFO - Progress: 41/704 files processed\n",
      "2025-09-08 19:53:27,672 - INFO - Progress: 51/704 files processed\n",
      "2025-09-08 19:53:27,679 - INFO - Progress: 61/704 files processed\n",
      "2025-09-08 19:53:27,816 - INFO - Progress: 71/704 files processed\n",
      "2025-09-08 19:53:28,770 - INFO - Progress: 81/704 files processed\n",
      "2025-09-08 19:53:28,779 - INFO - Progress: 91/704 files processed\n",
      "2025-09-08 19:53:28,780 - INFO - Progress: 101/704 files processed\n",
      "2025-09-08 19:53:28,780 - INFO - Progress: 111/704 files processed\n",
      "2025-09-08 19:53:28,971 - INFO - Progress: 121/704 files processed\n",
      "2025-09-08 19:53:29,202 - INFO - Progress: 131/704 files processed\n",
      "2025-09-08 19:53:29,420 - INFO - Progress: 141/704 files processed\n",
      "2025-09-08 19:53:30,516 - INFO - Progress: 151/704 files processed\n",
      "2025-09-08 19:53:30,976 - INFO - Progress: 161/704 files processed\n",
      "2025-09-08 19:53:31,232 - INFO - Progress: 171/704 files processed\n",
      "2025-09-08 19:53:31,843 - INFO - Progress: 181/704 files processed\n",
      "2025-09-08 19:53:31,843 - INFO - Progress: 191/704 files processed\n",
      "2025-09-08 19:53:31,844 - INFO - Progress: 201/704 files processed\n",
      "2025-09-08 19:53:32,626 - INFO - Progress: 211/704 files processed\n",
      "2025-09-08 19:53:33,185 - INFO - Progress: 221/704 files processed\n",
      "2025-09-08 19:53:33,221 - INFO - Progress: 231/704 files processed\n",
      "2025-09-08 19:53:33,826 - INFO - Progress: 241/704 files processed\n",
      "2025-09-08 19:53:33,827 - INFO - Progress: 251/704 files processed\n",
      "2025-09-08 19:53:34,633 - INFO - Progress: 261/704 files processed\n",
      "2025-09-08 19:53:34,998 - INFO - Progress: 271/704 files processed\n",
      "2025-09-08 19:53:34,998 - INFO - Progress: 281/704 files processed\n",
      "2025-09-08 19:53:35,958 - INFO - Progress: 291/704 files processed\n",
      "2025-09-08 19:53:36,446 - INFO - Progress: 301/704 files processed\n",
      "2025-09-08 19:53:36,641 - INFO - Progress: 311/704 files processed\n",
      "2025-09-08 19:53:36,642 - INFO - Progress: 321/704 files processed\n",
      "2025-09-08 19:53:36,643 - INFO - Progress: 331/704 files processed\n",
      "2025-09-08 19:53:37,739 - INFO - Progress: 341/704 files processed\n",
      "2025-09-08 19:53:37,740 - INFO - Progress: 351/704 files processed\n",
      "2025-09-08 19:53:37,755 - INFO - Progress: 361/704 files processed\n",
      "2025-09-08 19:53:37,765 - INFO - Progress: 371/704 files processed\n",
      "2025-09-08 19:53:38,804 - INFO - Progress: 381/704 files processed\n",
      "2025-09-08 19:53:38,805 - INFO - Progress: 391/704 files processed\n",
      "2025-09-08 19:53:38,806 - INFO - Progress: 401/704 files processed\n",
      "2025-09-08 19:53:38,807 - INFO - Progress: 411/704 files processed\n",
      "2025-09-08 19:53:39,046 - INFO - Progress: 421/704 files processed\n",
      "2025-09-08 19:53:39,302 - INFO - Progress: 431/704 files processed\n",
      "2025-09-08 19:53:39,496 - INFO - Progress: 441/704 files processed\n",
      "2025-09-08 19:53:39,704 - INFO - Progress: 451/704 files processed\n",
      "2025-09-08 19:53:39,910 - INFO - Progress: 461/704 files processed\n",
      "2025-09-08 19:53:41,036 - INFO - Progress: 471/704 files processed\n",
      "2025-09-08 19:53:41,036 - INFO - Progress: 481/704 files processed\n",
      "2025-09-08 19:53:41,038 - INFO - Progress: 491/704 files processed\n",
      "2025-09-08 19:53:41,038 - INFO - Progress: 501/704 files processed\n",
      "2025-09-08 19:53:41,174 - INFO - Progress: 511/704 files processed\n",
      "2025-09-08 19:53:42,270 - INFO - Progress: 521/704 files processed\n",
      "2025-09-08 19:53:42,273 - INFO - Progress: 531/704 files processed\n",
      "2025-09-08 19:53:42,715 - INFO - Progress: 541/704 files processed\n",
      "2025-09-08 19:53:42,716 - INFO - Progress: 551/704 files processed\n",
      "2025-09-08 19:53:42,720 - INFO - Progress: 561/704 files processed\n",
      "2025-09-08 19:53:42,735 - INFO - Progress: 571/704 files processed\n",
      "2025-09-08 19:53:42,979 - INFO - Progress: 581/704 files processed\n",
      "2025-09-08 19:53:43,341 - INFO - Progress: 591/704 files processed\n",
      "2025-09-08 19:53:43,559 - INFO - Progress: 601/704 files processed\n",
      "2025-09-08 19:53:43,784 - INFO - Progress: 611/704 files processed\n",
      "2025-09-08 19:53:44,007 - INFO - Progress: 621/704 files processed\n",
      "2025-09-08 19:53:44,245 - INFO - Progress: 631/704 files processed\n",
      "2025-09-08 19:53:44,458 - INFO - Progress: 641/704 files processed\n",
      "2025-09-08 19:53:44,673 - INFO - Progress: 651/704 files processed\n",
      "2025-09-08 19:53:44,884 - INFO - Progress: 661/704 files processed\n",
      "2025-09-08 19:53:45,149 - INFO - Progress: 671/704 files processed\n",
      "2025-09-08 19:53:45,358 - INFO - Progress: 681/704 files processed\n",
      "2025-09-08 19:53:45,589 - INFO - Progress: 691/704 files processed\n",
      "2025-09-08 19:53:46,514 - INFO - Progress: 701/704 files processed\n",
      "2025-09-08 19:53:46,515 - INFO - Edition 'Padova_1618_Cesare_Ripa' complete:\n",
      "2025-09-08 19:53:46,515 - INFO -   Total files: 704\n",
      "2025-09-08 19:53:46,515 - INFO -   Processed: 667\n",
      "2025-09-08 19:53:46,515 - INFO -   Skipped (clean): 37\n",
      "2025-09-08 19:53:46,516 - INFO -   Failed: 0\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'early_exit': True,           # Would catch this as already clean\n",
    "    'denoise': False,             # Skip for clean scans\n",
    "    'clahe_limit': 1.0,          # Very light if any\n",
    "    'remove_shadows': False,      # Not needed here\n",
    "    'deskew': True,              # Only if needed\n",
    "    'threshold': False           # Skip for clean prints\n",
    "}\n",
    "\n",
    "editions_path = Path(\"Editions\")\n",
    "result = process_edition(editions_path, \"Padova_1618_Cesare_Ripa\", options, max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1bacbc-d98d-4df7-a8e4-6124ea57f94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d160b-2c1a-4ef9-be8b-83c057a5103e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78fb3e3-6f64-42c1-8bbb-cc72b27fcfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage examples:\n",
    "\n",
    "# Preprocess single PDF folder\n",
    "# preprocess_images(\"Editions/Padova_1618_Cesare_Ripa\", deskew=True, enhance_contrast=True, contrast_clip=2.5)\n",
    "\n",
    "# Preprocess all folders in Editions/\n",
    "# preprocess_editions_folder(deskew=True, enhance_contrast=True, contrast_clip=2.5, denoise=True)\n",
    "\n",
    "# Heavy preprocessing for challenging documents\n",
    "# preprocess_editions_folder(deskew=True, enhance_contrast=True, contrast_clip=3.0, denoise=True, denoise_strength=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
